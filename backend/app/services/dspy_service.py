import dspy
import pandas as pd
import io
import json
from typing import List, Dict, Any

# 1. Define signatures for different tasks
class BasicQASignature(dspy.Signature):
    """Answer the question."""
    question = dspy.InputField()
    answer = dspy.OutputField()

class QualityJudgeSignature(dspy.Signature):
    """Evaluate answer quality using LLM-as-a-Judge."""
    question = dspy.InputField()
    generated_answer = dspy.InputField()
    ground_truth_answer = dspy.InputField()
    quality_score = dspy.OutputField(desc="Quality score from 1-5")
    feedback = dspy.OutputField(desc="Constructive feedback on the answer quality")

class DspyService:
    def configure_llm(self, provider: str, model: str, api_key: str):
        """Configures the DSPy LLM based on the selected provider."""
        if provider == "ollama":
            # For Ollama, the model name is passed directly
            llm = dspy.LM(model=model, max_tokens=150, api_base='http://localhost:11434', api_key='')
        elif provider == "openrouter":
            # OpenRouter uses an OpenAI-compatible API
            if not api_key:
                raise ValueError("API key is required for OpenRouter.")
            llm = dspy.OpenAI(
                model=model, # e.g., "meta-llama/llama-3-8b-instruct"
                api_key=api_key,
                api_base="https://openrouter.ai/api/v1",
                max_tokens=150
            )
        elif provider == "groq":
            # Groq also uses an OpenAI-compatible API
            if not api_key:
                raise ValueError("API key is required for Groq.")
            llm = dspy.OpenAI(
                model=model, # e.g., "llama3-8b-8192"
                api_key=api_key,
                api_base="https://api.groq.com/openai/v1",
                max_tokens=150
            )
        else:
            raise ValueError(f"Unsupported provider: {provider}")
        
        dspy.settings.configure(lm=llm)

    def llm_as_a_judge_metric(self, generated_answer: str, ground_truth_answer: str, question: str = "") -> float:
        """
        LLM-as-a-Judge metric for qualitative evaluation.

        Args:
            generated_answer: The answer generated by the prompt
            ground_truth_answer: The expected/correct answer
            question: The original question (optional context)

        Returns:
            Quality score between 0.0 and 1.0
        """
        try:
            # Create the judge program
            judge_program = dspy.Predict(QualityJudgeSignature)

            # Get LLM evaluation
            result = judge_program(
                question=question,
                generated_answer=generated_answer,
                ground_truth_answer=ground_truth_answer
            )

            # Parse quality score (expecting 1-5 range)
            try:
                raw_score = float(result.quality_score)
                # Normalize to 0-1 range
                normalized_score = max(0.0, min(1.0, raw_score / 5.0))
                return normalized_score
            except (ValueError, AttributeError):
                # If parsing fails, return a neutral score
                return 0.5

        except Exception as e:
            print(f"LLM-as-a-Judge evaluation failed: {e}")
            # Return neutral score on failure
            return 0.5

    def optimize_prompt(self, original_prompt: str, file_content: bytes, filename: str,
                       provider: str, model: str, api_key: str, metric: str = "exact_match") -> str:
         # 1. Configure the LLM for this specific request
        self.configure_llm(provider, model, api_key)

        # 2. Load the dataset
        if filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(file_content))
        elif filename.endswith('.jsonl'):
            df = pd.read_json(io.BytesIO(file_content), lines=True)
        else:
            raise ValueError("Unsupported file type. Please use .csv or .jsonl")
        
        train_set = [dspy.Example(question=row['question'], answer=row['answer']).with_inputs('question') for _, row in df.iterrows()]

        # 2. Define the program to optimize
        class BasicQAProgram(dspy.Module):
            def __init__(self):
                super().__init__()
                self.predictor = dspy.Predict(BasicQASignature, instructions=original_prompt)

            def forward(self, question):
                return self.predictor(question=question)

        # 3. Set up the optimizer with the selected metric
        from dspy.teleprompt import BootstrapFewShot

        if metric == "llm_as_a_judge":
            # Use LLM-as-a-Judge metric for qualitative evaluation
            def llm_judge_metric(example, pred, trace=None):
                question = example.question
                ground_truth = example.answer
                generated_answer = pred.answer
                return self.llm_as_a_judge_metric(generated_answer, ground_truth, question)

            config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)
            teleprompter = BootstrapFewShot(metric=llm_judge_metric, **config)
        else:
            # Default to exact match metric
            config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)
            teleprompter = BootstrapFewShot(metric=dspy.evaluate.answer_exact_match, **config)

        optimized_program = teleprompter.compile(BasicQAProgram(), trainset=train_set)

        # 4. Retrieve the learned demonstrations from the optimized predictor
        demos = optimized_program.predictor.demos
        
        # Format the demos into a human-readable string
        example_string = ""
        for demo in demos:
            example_string += f"Question: {demo.question}\n"
            example_string += f"Answer: {demo.answer}\n\n"
            
        # The new "optimized prompt" is the original instruction plus the learned examples
        optimized_prompt = f"{original_prompt}\n\n--- Examples ---\n{example_string}"
        
        return optimized_prompt